#Author : Rohan M. Nanaware
#Date C.: 25th Jun 2017
#Date M.: 25th Jun 2017
#       : 28th Jun 2017 - Random forest 1st iteration with NA imputation
#       : 29th Jun 2017 - XGBoost - NA imputation, adjusted eta, depth and iterations. Ran model only on significant factors 
#       : 30th Jun 2017 - Feature engineering | List of features added - 
#       : 01st Jul 2017 - Random forest | Run on the feature engineered dataset
#                       - Understand the reason why XGBoost iteration failed
#Purpose: Check quality of ML knowledge using the Titanic dataset | Leaderbard score on Kaggle

{
  library(data.table)
  library(Matrix)
  library(rpart)
  library(xgboost)
  library(readr)
  library(stringr)
  library(caret)
  library(car)
  library(data.table)
  library(randomForest)
}#01. Load required packages
{
wd <- "E:/Delivery/1 Active/Kaggle/Titanic Machine Learning from Disaster"
setwd(wd)
}#02. Set working directory
{
  train <- read.csv("train.csv", header = T, stringsAsFactors = F)
  test  <- read.csv("test.csv", header = T, stringsAsFactors = F)
  RMS.TOTAL <- rbind(train[!colnames(train) %in% c("Survived")], test)
  sum(!complete.cases(RMS.TOTAL))#Rows with NAs - 264
  sapply(RMS.TOTAL, function(x) sum(is.na(x)))#Age - 263, Fare - 1
}#03. Load data
{
  #View(head(RMS.TOTAL))
  #Impute Age by prediction
  summary(RMS.TOTAL$Age, na.rm = T)
  PREDICT_AGE <- rpart(Age ~ Pclass+Sex+SibSp+Parch+Fare+Embarked,
                       data = RMS.TOTAL[complete.cases(RMS.TOTAL),],
                       method = "anova")
  RMS.TOTAL$Age[is.na(RMS.TOTAL$Age)] <- predict(PREDICT_AGE, RMS.TOTAL[is.na(RMS.TOTAL$Age),])
  summary(RMS.TOTAL$Age, na.rm = T)
  #Impute fare by mean
  summary(RMS.TOTAL$Fare)
  RMS.TOTAL <- data.table(RMS.TOTAL)
  FARE.EM.CLASS <- RMS.TOTAL[, (MEAN_FARE = median(Fare, na.rm = T)),
                             by = 'Pclass'] 
  RMS.TOTAL$Pclass[is.na(RMS.TOTAL$Fare)]
  RMS.TOTAL$Fare[is.na(RMS.TOTAL$Fare)] = FARE.EM.CLASS$V1[FARE.EM.CLASS$Pclass == RMS.TOTAL$Pclass[is.na(RMS.TOTAL$Fare)]]
  summary(RMS.TOTAL$Fare)
  sapply(RMS.TOTAL, function(x) sum(is.na(x)))
  #check for presence of quotes in any of the factors
  nrow(RMS.TOTAL[RMS.TOTAL$Embarked == "" ])
  #Cabin    - 1014
  #Embarked - 2
  #Based on secondary research - Miss Rose AmÃ©lie Icard &  Mrs George Nelson Stone 
  #travelled on Mrs Stone's ticket (#113572). They embarked at Southampton
  RMS.TOTAL$Embarked[RMS.TOTAL$Embarked == "" ] <- "S"
  rm(PREDICT_AGE, FARE.EM.CLASS)
}#04. Impute NAs
{
RMS.TRAIN <- as.data.frame(cbind(RMS.TOTAL[1:nrow(train),], train$Survived))
RMS.TEST  <- as.data.frame(RMS.TOTAL[-(1:nrow(train)),])
colnames(RMS.TRAIN)[colnames(RMS.TRAIN) == "V2"] <- 'Survived'
RMS.TRAIN.7 <- RMS.TRAIN[1:as.integer(nrow(RMS.TRAIN)*0.7),]
RMS.TRAIN.3 <- RMS.TRAIN[-(1:as.integer(nrow(RMS.TRAIN)*0.7)),]
}#05. Split up train and test data
{
RMS.TRAIN$Pclass   <- as.factor(RMS.TRAIN$Pclass)
RMS.TRAIN$Sex      <- as.factor(RMS.TRAIN$Sex)
RMS.TRAIN$Embarked <- as.factor(RMS.TRAIN$Embarked)
RMS.TRAIN$Survived <- as.factor(RMS.TRAIN$Survived)
RMS.TRAIN.7 <- RMS.TRAIN[1:as.integer(nrow(RMS.TRAIN)*0.7),]
RMS.TRAIN.3 <- RMS.TRAIN[-(1:as.integer(nrow(RMS.TRAIN)*0.7)),]
RF_1        <- randomForest(Survived ~ Pclass+
                              Sex+
                              Age+
                              SibSp+
                              Parch+
                              Fare+
                              Embarked,
                            data = data.frame(RMS.TRAIN.7),
                            importance = T,
                            ntree = 1000)
PREDICTION <- data.frame(Survived = predict(RF_1, RMS.TRAIN.7))
table(PREDICTION$Survived, RMS.TRAIN.7$Survived)#82%
varImpPlot(RF_1)
#Test
PREDICTION.3 <- data.frame(Survived = predict(RF_1, RMS.TRAIN.3))
table(PREDICTION.3$Survived, RMS.TRAIN.3$Survived)#82%
varImpPlot(RF_1)
#ITERATION 2 - Only considered significant variables on complete training dataset
RF_2        <- randomForest(Survived ~ Pclass+
                              Sex+
                              Age+
                              SibSp+
                              #Parch+
                              Fare,
                            #Embarked,
                            data = RMS.TRAIN,
                            importance = T,
                            ntree = 1000)
PREDICTION <- data.frame(Survived = predict(RF_2, RMS.TRAIN))
RF_2$confusion
varImpPlot(RF_2)
#Prediction on test data
PREDICTION.TEST <- data.frame(Survived = predict(RF_2, RMS.TRAIN.3))
table(PREDICTION.TEST$Survived, RMS.TRAIN.3$Survived)

#Kaggle test data
test$Pclass   <- as.factor(test$Pclass)
test$Sex      <- as.factor(test$Sex)
test$Embarked <- as.factor(test$Embarked)
RMS.TEST$Pclass   <- as.factor(RMS.TEST$Pclass)
RMS.TEST$Sex      <- as.factor(RMS.TEST$Sex)
RMS.TEST$Embarked <- as.factor(RMS.TEST$Embarked)

PREDICTION.TEST <- data.frame(Survived = predict(RF_2, RMS.TEST))
RESULT <- data.frame(PassengerId = RMS.TEST$PassengerId, Survived = PREDICTION.TEST$Survived)
write.csv(RESULT, "170628_RMS_ITER1.csv", row.names = F)
#Train accuracy - 82.15%
#Rank - 3741
}#06. Modelling technique - Random forest
{
sapply(RMS.TRAIN.7, function(x) length(unique(x)))
#Filter out unrequired columns - 
REDUCE_COLS <- c("PassengerId","Name","Ticket","Cabin")
RMS.TRAIN.7 <- data.table(RMS.TRAIN.7)
RMS.TRAIN.7 <- RMS.TRAIN.7[, !REDUCE_COLS, with = FALSE] 
FEATURES_OHE<- c("Pclass",
                 "Sex",
                 "SibSp",
                 "Embarked")
DUMMIES     <- dummyVars(~ Pclass+
                           Sex+
                           SibSp+
                           Embarked,
                           data = RMS.TRAIN.7)
RMS.TRAIN.7.OHE <- as.data.frame(predict(DUMMIES, newdata = RMS.TRAIN.7))
RMS.TRAIN.7     <- data.frame(RMS.TRAIN.7)
RMS.TRAIN.7.ENC <- cbind(RMS.TRAIN.7[,-c(which(colnames(RMS.TRAIN.7) %in% FEATURES_OHE))], RMS.TRAIN.7.OHE)
RMS.TRAIN.7.ENC[] <- lapply(RMS.TRAIN.7.ENC, as.numeric)
RMS.TRAIN.7.ENC$Survived <- ifelse(RMS.TRAIN.7.ENC$Survived == 1, 0, 1)
RMS.TRAIN.7.ENC.F <- RMS.TRAIN.7.ENC[,which(colnames(RMS.TRAIN.7.ENC) != "Survived")] 
#RMS.TRAIN.7.ENC.F <- RMS.TRAIN.7.ENC.F[, c(colnames(RMS.TRAIN.7.ENC.F) %in% FILTER_COLS)]
#Run XGB
set.seed(1007)
XGB_1 <- xgboost(data = data.matrix(RMS.TRAIN.7.ENC.F),
                 label = RMS.TRAIN.7.ENC$Survived,
                 eta = 0.01,
                 max_depth = 3,
                 nround = 2000,
                 seed = 1007,
                 objective = "binary:logistic",
                 nthread = 2
                 )
#test
REDUCE_COLS <- c("PassengerId","Name","Ticket","Cabin")
RMS.TRAIN.3 <- data.table(RMS.TRAIN.3)
RMS.TRAIN.3 <- RMS.TRAIN.3[, !REDUCE_COLS, with = FALSE] 
FEATURES_OHE<- c("Pclass",
                 "Sex",
                 "SibSp",
                 "Embarked")
DUMMIES     <- dummyVars(~ Pclass+
                           Sex+
                           SibSp+
                           Embarked,
                         data = RMS.TRAIN.3)
RMS.TRAIN.3.OHE <- as.data.frame(predict(DUMMIES, newdata = RMS.TRAIN.3))
RMS.TRAIN.3     <- data.frame(RMS.TRAIN.3)
RMS.TRAIN.3.ENC <- cbind(RMS.TRAIN.3[,-c(which(colnames(RMS.TRAIN.3) %in% FEATURES_OHE))], RMS.TRAIN.3.OHE)
RMS.TRAIN.3.ENC[] <- lapply(RMS.TRAIN.3.ENC, as.numeric)
RMS.TRAIN.3.ENC$Survived <- ifelse(RMS.TRAIN.3.ENC$Survived == 1, 0, 1)
RMS.TRAIN.3.ENC.F <- RMS.TRAIN.3.ENC[,which(colnames(RMS.TRAIN.3.ENC) != "Survived")] 

PREDICTION <- predict(XGB_1, data.matrix(RMS.TRAIN.3.ENC.F))
PREDICTION <- data.frame(PREDICTION = as.numeric(PREDICTION > 0.5))
confusionMatrix(PREDICTION$PREDICTION, RMS.TRAIN.3.ENC$Survived)

library(ROCR)
data("ROCR.simple")
data(RM)
PREDICTION <- data.frame(PREDICTION = predict(XGB_1, data.matrix(RMS.TRAIN.7.ENC.F)))
pred <- prediction(PREDICTION$PREDICTION, RMS.TRAIN.7.ENC$Survived)
ss <- performance(pred, "sens", "spec")
plot(ss)
ss@alpha.values[[1]][which.max(ss@x.values[[1]]+ss@y.values[[1]])]
#[1] 0.3934746
PREDICTION <- predict(XGB_1, data.matrix(RMS.TRAIN.7.ENC.F))
PREDICTION <- data.frame(PREDICTION = as.numeric(PREDICTION > 0.39))
confusionMatrix(PREDICTION$PREDICTION, RMS.TRAIN.7.ENC$Survived)

#Train on complete sample
sapply(RMS.TRAIN, function(x) length(unique(x)))

#Filter out unrequired columns - 
REDUCE_COLS <- c("PassengerId","Name","Ticket","Cabin")
RMS.TRAIN <- data.table(RMS.TRAIN)
RMS.TRAIN <- RMS.TRAIN[, !REDUCE_COLS, with = FALSE] 
FEATURES_OHE<- c("Pclass",
                 "Sex",
                 "SibSp",
                 "Embarked")
DUMMIES     <- dummyVars(~ Pclass+
                           Sex+
                           SibSp+
                           Embarked,
                         data = RMS.TRAIN)
RMS.TRAIN.OHE <- as.data.frame(predict(DUMMIES, newdata = RMS.TRAIN))
RMS.TRAIN     <- data.frame(RMS.TRAIN)
RMS.TRAIN.ENC <- cbind(RMS.TRAIN[,-c(which(colnames(RMS.TRAIN) %in% FEATURES_OHE))], RMS.TRAIN.OHE)
RMS.TRAIN.ENC[] <- lapply(RMS.TRAIN.ENC, as.numeric)
RMS.TRAIN.ENC$Survived <- ifelse(RMS.TRAIN.ENC$Survived == 1, 0, 1)
RMS.TRAIN.ENC.F <- RMS.TRAIN.ENC[,which(colnames(RMS.TRAIN.ENC) != "Survived")] 

#Run XGB
set.seed(1007)
XGB_2 <- xgboost(data = data.matrix(RMS.TRAIN.ENC.F),
                 label = RMS.TRAIN.ENC$Survived,
                 eta = 0.01,
                 max_depth = 5,
                 nround = 1000,
                 seed = 1007,
                 objective = "binary:logistic",
                 nthread = 2
)

PREDICTION <- predict(XGB_2, data.matrix(RMS.TRAIN.ENC.F))
PREDICTION <- data.frame(PREDICTION = as.numeric(PREDICTION > 0.5))
confusionMatrix(PREDICTION$PREDICTION, RMS.TRAIN.ENC$Survived)

#Kaggle test data
REDUCE_COLS <- c("Pa0ssengerId","Name","Ticket","Cabin")
RMS.TEST <- data.table(RMS.TEST)
RMS.TEST <- RMS.TEST[, !REDUCE_COLS, with = FALSE] 
FEATURES_OHE<- c("Pclass",
                 "Sex",
                 "SibSp",
                 "Embarked")
DUMMIES     <- dummyVars(~ Pclass+
                           Sex+
                           SibSp+
                           Embarked,
                         data = RMS.TEST)
RMS.TEST.OHE <- as.data.frame(predict(DUMMIES, newdata = RMS.TEST))
RMS.TEST     <- data.frame(RMS.TEST)
RMS.TEST.ENC <- cbind(RMS.TEST[,-c(which(colnames(RMS.TEST) %in% FEATURES_OHE))], RMS.TEST.OHE)
RMS.TEST.ENC[] <- lapply(RMS.TEST.ENC, as.numeric)
RMS.TEST.ENC$Survived <- ifelse(RMS.TEST.ENC$Survived == 1, 0, 1)
RMS.TEST.ENC.F <- RMS.TEST.ENC[,which(colnames(RMS.TEST.ENC) != "Survived")] 

PREDICTION <- predict(XGB_2, data.matrix(RMS.TEST.ENC.F))
PREDICTION <- data.frame(PREDICTION = as.numeric(PREDICTION > 0.5))
RESULT     <- cbind(PassengerId = RMS.TEST$PassengerId, Survived = PREDICTION$PREDICTION)
write.csv(RESULT, "170629_RMS_ITER2.csv", row.names = F)

#Train accuracy - 90.68%
#Leaderboard rank - 2347

names <- dimnames(data.matrix(RMS.TRAIN.3.ENC.F))[[2]]
importance_matrix <- xgb.importance(names, model = XGB_1)
xgb.plot.importance(importance_matrix)
FILTER_COLS <- importance_matrix$Feature[cumsum(importance_matrix$Gain) <= 0.99]
RMS.TRAIN.7.ENC.F <- RMS.TRAIN.7.ENC.F[, c(colnames(RMS.TRAIN.7.ENC.F) %in% FILTER_COLS)]
}#07. Modelling technique - XGBoost
{
sapply(RMS.TOTAL, function(x) sum(is.na(x)))
head(RMS.TOTAL)

#Get the surnames
RMS.TOTAL$Surname <- sapply(as.character(RMS.TOTAL$Name), FUN = 
                              function(x) {strsplit(x, '[,.]')}[[1]][1])
#Compute ticket, fare and cabin frequencies
RMS.TOTAL$T.FREQ  <- ave(seq(nrow(RMS.TOTAL)), RMS.TOTAL$Ticket, FUN = length)
RMS.TOTAL$F.FREQ  <- ave(seq(nrow(RMS.TOTAL)), RMS.TOTAL$Fare, FUN = length)
RMS.TOTAL$C.FREQ  <- ave(seq(nrow(RMS.TOTAL)), RMS.TOTAL$Cabin, FUN = length)

#Define group IDs
RMS.TOTAL$GRP.ID <- rep(NA, nrow(RMS.TOTAL))
MAX.GRP          <- 12
for (i in 1:nrow(RMS.TOTAL)) {
  if (RMS.TOTAL$SibSp[i] + RMS.TOTAL$Parch[i] > 0){#Doe he have any relatives
    RMS.TOTAL$GRP.ID[i] <- paste0(RMS.TOTAL$Surname[i], RMS.TOTAL$SibSp[i] + RMS.TOTAL$Parch[i])
  }
  else {
    if (RMS.TOTAL$T.FREQ[i] > 1 & is.na(RMS.TOTAL$GRP.ID[i])){#shares ticket
      RMS.TOTAL$GRP.ID[i] <- RMS.TOTAL$Ticket[i]
    }
    else {
      if(RMS.TOTAL$C.FREQ[i] > 1 & RMS.TOTAL$C.FREQ[i] < MAX.GRP & is.na(RMS.TOTAL$GRP.ID[i])){
        RMS.TOTAL$GRP.ID[i] <- RMS.TOTAL$Cabin[i]#shares cabin
      }
      else {
        if(RMS.TOTAL$F.FREQ[i] > 1 & RMS.TOTAL$F.FREQ[i] < MAX.GRP & is.na(RMS.TOTAL$GRP.ID[i])){
          RMS.TOTAL$GRP.ID[i] <- RMS.TOTAL$Fare[i]#shares fare
        }
        else {
          RMS.TOTAL$GRP.ID[i] <- "Single"
        }
      }
    }
  }
}
View(RMS.TOTAL[RMS.TOTAL$GRP.ID == '6.75'])

#Define fare per person for people sharing tickets
RMS.TOTAL$FARE.SCALED <- RMS.TOTAL$Fare/RMS.TOTAL$T.FREQ

#Check the reliability of age information across pclass
age_plot <- ggplot(RMS.TOTAL, aes(Pclass, fill = !is.na(Age))) + 
  geom_bar(position = 'dodge')+
  labs(title = 'Passenger has age', fill = 'His Age')
#define log likelihood and penalize or reawrd passengers
}#08. Feature engineering 1
{
#Extract title - 
RMS.TOTAL$Title <- gsub('(.*, )|(\\..*)', '', RMS.TOTAL$Name)
table(RMS.TOTAL$Title, RMS.TOTAL$Sex)
#combine titles with low cell counts
RARE.TITLE <- c('Dona', 'Lady', 'the Countess', 'Capt',
                'Col','Don', 'Dr', 'Major', 'Rev', 'Sir',
                'Jonkheer')
RMS.TOTAL$Title[RMS.TOTAL$Title == 'Mlle'] = 'Miss'
RMS.TOTAL$Title[RMS.TOTAL$Title == 'Ms']   = 'Miss'
RMS.TOTAL$Title[RMS.TOTAL$Title == 'Mme']  = 'Mrs'
RMS.TOTAL$Title[RMS.TOTAL$Title %in% RARE.TITLE] = 'Rare'
table(RMS.TOTAL$Title, RMS.TOTAL$Sex)
#Add ethnicity details using surname?
RMS.TOTAL$Surname <- sapply(as.character(RMS.TOTAL$Name), FUN = 
                              function(x) {strsplit(x, '[,.]')}[[1]][1])
RMS.TOTAL$F.SIZE  <- RMS.TOTAL$SibSp + RMS.TOTAL$Parch + 1
RMS.TOTAL$Family  <- paste(RMS.TOTAL$Surname, RMS.TOTAL$F.SIZE, sep = "_")
RMS.TOTAL$F.SIZE.B[RMS.TOTAL$F.SIZE == 1] <- 'Singleton'
RMS.TOTAL$F.SIZE.B[RMS.TOTAL$F.SIZE < 5 & RMS.TOTAL$F.SIZE > 1] <- 'Small'
RMS.TOTAL$F.SIZE.B[RMS.TOTAL$F.SIZE >= 5] <- 'Large'
#Create a deck variable
RMS.TOTAL$DECK <- sapply(RMS.TOTAL$Cabin, FUN = function(x) {strsplit(x, NULL)}[[1]][1])
#RMS.TOTAL$DECK <- NULL
RMS.TOTAL$CHILD[RMS.TOTAL$Age < 18] <- 'Child'
RMS.TOTAL$CHILD[RMS.TOTAL$Age >= 18] <- 'Adult'
RMS.TOTAL$MOTHER <- 'Not Mother' 
RMS.TOTAL$MOTHER[RMS.TOTAL$Sex == 'female' & RMS.TOTAL$Age > 18 & 
                   RMS.TOTAL$Parch > 0 & RMS.TOTAL$Title != 'Miss'] <- 'Mother'
colnames(RMS.TOTAL)
RMS.TOTAL$Surname <- NULL
#RMS.TOTAL$F.SIZE  <- NULL
RMS.TOTAL$Family  <- NULL
RMS.TOTAL$DECK    <- NULL
rm(RARE.TITLE)
}#09. Feature engineering 2
{
#Split up train and test data
RMS.TRAIN <- as.data.frame(cbind(RMS.TOTAL[1:nrow(train),], train$Survived))
RMS.TEST  <- as.data.frame(RMS.TOTAL[-(1:nrow(train)),])
colnames(RMS.TRAIN)[colnames(RMS.TRAIN) == 'V2'] <- 'Survived'

RMS.TRAIN.7 <- RMS.TRAIN[1:as.integer(nrow(RMS.TRAIN)*0.7),]
RMS.TRAIN.3 <- RMS.TRAIN[-(1:as.integer(nrow(RMS.TRAIN)*0.7)),]

#Filter out unrequired columns - 
REDUCE_COLS <- c("PassengerId","Name","Ticket","Cabin","Surname","Family")
RMS.TRAIN.7 <- data.table(RMS.TRAIN.7)
RMS.TRAIN.7 <- RMS.TRAIN.7[, !REDUCE_COLS, with = FALSE] 
FEATURES_OHE<- c("Pclass",
                 "Sex",
                 "Embarked",
                 "Title",
                 "F.SIZE.B",
                 "CHILD",
                 "MOTHER"
)
DUMMIES     <- dummyVars(~ Pclass+
                           Sex+
                           Embarked+
                           Title+
                           F.SIZE.B+
                           CHILD+
                           MOTHER,
                         data = RMS.TRAIN.7)
RMS.TRAIN.7.OHE <- as.data.frame(predict(DUMMIES, newdata = RMS.TRAIN.7))
RMS.TRAIN.7     <- data.frame(RMS.TRAIN.7)
RMS.TRAIN.7.ENC <- cbind(RMS.TRAIN.7[,-c(which(colnames(RMS.TRAIN.7) %in% FEATURES_OHE))], RMS.TRAIN.7.OHE)
RMS.TRAIN.7.ENC[] <- lapply(RMS.TRAIN.7.ENC, as.numeric)
RMS.TRAIN.7.ENC$Survived <- ifelse(RMS.TRAIN.7.ENC$Survived == 1, 0, 1)
RMS.TRAIN.7.ENC.F <- RMS.TRAIN.7.ENC[,which(colnames(RMS.TRAIN.7.ENC) != "Survived")] 
names <- dimnames(data.matrix(RMS.TRAIN.ENC.F))[[2]]
importance_matrix <- xgb.importance(names, model = XGB_3)
FILTER_COLS <- importance_matrix$Feature[cumsum(importance_matrix$Gain) <= 0.99]
RMS.TRAIN.7.ENC.F <- RMS.TRAIN.7.ENC.F[, c(colnames(RMS.TRAIN.7.ENC.F) %in% FILTER_COLS)]
#Run XGB
set.seed(1007)
XGB_3 <- xgboost(data = data.matrix(RMS.TRAIN.7.ENC.F),
                 label = RMS.TRAIN.7.ENC$Survived,
                 eta = 0.01,
                 max_depth = 5,
                 nround = 2000,
                 seed = 1007,
                 objective = "binary:logistic",
                 nthread = 2
)
#test
RMS.TRAIN.3 <- data.table(RMS.TRAIN.3)
RMS.TRAIN.3 <- RMS.TRAIN.3[, !REDUCE_COLS, with = FALSE] 
DUMMIES     <- dummyVars(~ Pclass+
                           Sex+
                           Embarked+
                           Title+
                           F.SIZE.B+
                           CHILD+
                           MOTHER,
                         data = RMS.TRAIN.3)
RMS.TRAIN.3.OHE <- as.data.frame(predict(DUMMIES, newdata = RMS.TRAIN.3))
RMS.TRAIN.3     <- data.frame(RMS.TRAIN.3)
RMS.TRAIN.3.ENC <- cbind(RMS.TRAIN.3[,-c(which(colnames(RMS.TRAIN.3) %in% FEATURES_OHE))], RMS.TRAIN.3.OHE)
RMS.TRAIN.3.ENC[] <- lapply(RMS.TRAIN.3.ENC, as.numeric)
RMS.TRAIN.3.ENC$Survived <- ifelse(RMS.TRAIN.3.ENC$Survived == 1, 0, 1)
RMS.TRAIN.3.ENC.F <- RMS.TRAIN.3.ENC[,which(colnames(RMS.TRAIN.3.ENC) != "Survived")] 
FILTER_COLS <- importance_matrix$Feature[cumsum(importance_matrix$Gain) <= 0.99]
RMS.TRAIN.3.ENC.F <- RMS.TRAIN.3.ENC.F[, c(colnames(RMS.TRAIN.3.ENC.F) %in% FILTER_COLS)]
PREDICTION <- predict(XGB_3, data.matrix(RMS.TRAIN.3.ENC.F))
PREDICTION <- data.frame(PREDICTION = as.numeric(PREDICTION > 0.5))
confusionMatrix(PREDICTION$PREDICTION, RMS.TRAIN.3.ENC$Survived)

#Train on complete data
#Split up train and test data
RMS.TRAIN <- as.data.frame(cbind(RMS.TOTAL[1:nrow(train),], train$Survived))
RMS.TEST  <- as.data.frame(RMS.TOTAL[-(1:nrow(train)),])
colnames(RMS.TRAIN)[colnames(RMS.TRAIN) == 'V2'] <- 'Survived'
#Filter out unrequired columns - 
REDUCE_COLS <- c("PassengerId","Name","Ticket","Cabin","Surname","Family")
RMS.TRAIN <- data.table(RMS.TRAIN)
RMS.TRAIN <- RMS.TRAIN[, !REDUCE_COLS, with = FALSE] 
FEATURES_OHE<- c("Pclass",
                 "Sex",
                 "Embarked",
                 "Title",
                 "F.SIZE.B",
                 "CHILD",
                 "MOTHER"
)
DUMMIES     <- dummyVars(~ Pclass+
                           Sex+
                           Embarked+
                           Title+
                           F.SIZE.B+
                           CHILD+
                           MOTHER,
                         data = RMS.TRAIN)
RMS.TRAIN.OHE <- as.data.frame(predict(DUMMIES, newdata = RMS.TRAIN))
RMS.TRAIN     <- data.frame(RMS.TRAIN)
RMS.TRAIN.ENC <- cbind(RMS.TRAIN[,-c(which(colnames(RMS.TRAIN) %in% FEATURES_OHE))], RMS.TRAIN.OHE)
RMS.TRAIN.ENC[] <- lapply(RMS.TRAIN.ENC, as.numeric)
RMS.TRAIN.ENC$Survived <- ifelse(RMS.TRAIN.ENC$Survived == 1, 0, 1)
RMS.TRAIN.ENC.F <- RMS.TRAIN.ENC[,which(colnames(RMS.TRAIN.ENC) != "Survived")] 
RMS.TRAIN.ENC.F$Embarked <- NULL
#Run XGB
set.seed(1007)
XGB_3 <- xgboost(data = data.matrix(RMS.TRAIN.ENC.F),
                 label = RMS.TRAIN.ENC$Survived,
                 eta = 0.01,
                 max_depth = 3,
                 nround = 2000,
                 seed = 1007,
                 objective = "binary:logistic",
                 nthread = 2
)
#test
RMS.TEST <- data.table(RMS.TEST)
RMS.TEST <- RMS.TEST[, !REDUCE_COLS, with = FALSE] 
DUMMIES     <- dummyVars(~ Pclass+
                           Sex+
                           Embarked+
                           Title+
                           F.SIZE.B+
                           CHILD+
                           MOTHER,
                         data = RMS.TEST)
RMS.TEST.OHE <- as.data.frame(predict(DUMMIES, newdata = RMS.TEST))
RMS.TEST     <- data.frame(RMS.TEST)
RMS.TEST.ENC <- cbind(RMS.TEST[,-c(which(colnames(RMS.TEST) %in% FEATURES_OHE))], RMS.TEST.OHE)
RMS.TEST.ENC[] <- lapply(RMS.TEST.ENC, as.numeric)
RMS.TEST.ENC.F <- RMS.TEST.ENC 
PREDICTION <- predict(XGB_3, data.matrix(RMS.TEST.ENC.F))
PREDICTION <- data.frame(PREDICTION = as.numeric(PREDICTION > 0.5))
RESULT     <- cbind(PassengerId = as.data.frame(RMS.TOTAL[-(1:nrow(train)),])$PassengerId, Survived = PREDICTION$PREDICTION)
write.csv(RESULT, "170630_RMS_ITER3.csv", row.names = F)
}#10. Modelling technique - XGBoost - 2
{
  #rm(list = ls())
  gc()
  #Module 03, 04, 09, 05
  #Train model
  #Prepare train data
  head(RMS.TRAIN.7)
  #Covert categorical to factors
  colnames(RMS.TRAIN.7)
  RMS.TRAIN.7[, colnames(RMS.TRAIN.7) == 'Pclass' |
                (sapply(RMS.TRAIN.7, class) == 'character' &
                   !(colnames(RMS.TRAIN.7) %in% c('Name','PassengerId','Ticket','Cabin')))] <-
    data.frame(
    sapply(RMS.TRAIN.7[, colnames(RMS.TRAIN.7) == 'Pclass' |
                          (sapply(RMS.TRAIN.7, class) == 'character' &
                          !(colnames(RMS.TRAIN.7) %in% c('Name','PassengerId','Ticket','Cabin')))],
           function(x) as.factor(x)
    ))
  RMS.TRAIN.7$Survived <- as.factor(RMS.TRAIN.7$Survived)
  RMS.TRAIN.7$CHILD  <- as.numeric(RMS.TRAIN.7$CHILD)
  RMS.TRAIN.7$MOTHER <- as.numeric(RMS.TRAIN.7$MOTHER) 
  RF_1        <- randomForest(Survived ~ Pclass+
                                Sex+
                                Age+
                                SibSp+
                                Parch+
                                Fare+
                                Embarked+
                                Title+
                                CHILD+
                                MOTHER+
                                F.SIZE,
                              data = data.frame(RMS.TRAIN.7),
                              importance = T,
                              ntree = 1000
                              #tuning
                              #nodesize = 15
                              #mtry = ncol(RMS.TRAIN.7)/2
                              )
  PREDICTION <- predict(RF_1, RMS.TRAIN.7)
  varImpPlot(RF_1)
  table(PREDICTION, RMS.TRAIN.7$Survived)#91% - 93.1% after removing Child, mother and family bucket size---overfit?
  ### - TEST - ###
  #Prepare test data
  head(RMS.TRAIN.3)
  #Covert categorical to factors
  colnames(RMS.TRAIN.3)
  RMS.TRAIN.3[, colnames(RMS.TRAIN.3) == 'Pclass' |
                (sapply(RMS.TRAIN.3, class) == 'character' &
                   !(colnames(RMS.TRAIN.3) %in% c('Name','PassengerId','Ticket','Cabin')))] <-
    data.frame(
      sapply(RMS.TRAIN.3[, colnames(RMS.TRAIN.3) == 'Pclass' |
                           (sapply(RMS.TRAIN.3, class) == 'character' &
                              !(colnames(RMS.TRAIN.3) %in% c('Name','PassengerId','Ticket','Cabin')))],
             function(x) as.factor(x)
      ))
  RMS.TRAIN.3$Survived <- as.factor(RMS.TRAIN.3$Survived)
  RMS.TRAIN.3$CHILD  <- as.numeric(RMS.TRAIN.3$CHILD)
  RMS.TRAIN.3$MOTHER <- as.numeric(RMS.TRAIN.3$MOTHER) 
  PREDICTION.3 <- data.frame(Survived = predict(RF_1, RMS.TRAIN.3))
  table(PREDICTION.3$Survived, RMS.TRAIN.3$Survived)#83.9%
  #ITERATION 2 - Only considered significant variables on complete training dataset
  #Prepare test data
  head(RMS.TRAIN)
  #Covert categorical to factors
  colnames(RMS.TRAIN)
  RMS.TRAIN[, colnames(RMS.TRAIN) == 'Pclass' |
                (sapply(RMS.TRAIN, class) == 'character' &
                   !(colnames(RMS.TRAIN) %in% c('Name','PassengerId','Ticket','Cabin')))] <-
    data.frame(
      sapply(RMS.TRAIN[, colnames(RMS.TRAIN) == 'Pclass' |
                           (sapply(RMS.TRAIN, class) == 'character' &
                              !(colnames(RMS.TRAIN) %in% c('Name','PassengerId','Ticket','Cabin')))],
             function(x) as.factor(x)
      ))
  RMS.TRAIN$Survived <- as.factor(RMS.TRAIN$Survived)
  RMS.TRAIN$CHILD  <- as.numeric(RMS.TRAIN$CHILD)
  RMS.TRAIN$MOTHER <- as.numeric(RMS.TRAIN$MOTHER) 
  RF_2        <- randomForest(Survived ~ Pclass+
                                Sex+
                                Age+
                                SibSp+
                                Parch+
                                Fare+
                                Embarked+
                                Title+
                                CHILD+
                                MOTHER+
                                F.SIZE,
                              data = data.frame(RMS.TRAIN),
                              importance = T,
                              ntree = 1000,
                              nodesize = 15)
  PREDICTION <- data.frame(Survived = predict(RF_2, RMS.TRAIN))
  PREDICTION.TEST <- data.frame(Survived = predict(RF_2, RMS.TRAIN))
  table(PREDICTION.TEST$Survived, RMS.TRAIN$Survived)#93.9%
  #Kaggle test data
  #Prepare test data
  head(RMS.TEST)
  #Covert categorical to factors
  colnames(RMS.TEST)
  RMS.TEST[, colnames(RMS.TEST) == 'Pclass' |
              (sapply(RMS.TEST, class) == 'character' &
                 !(colnames(RMS.TEST) %in% c('Name','PassengerId','Ticket','Cabin')))] <-
    data.frame(
      sapply(RMS.TEST[, colnames(RMS.TEST) == 'Pclass' |
                         (sapply(RMS.TEST, class) == 'character' &
                            !(colnames(RMS.TEST) %in% c('Name','PassengerId','Ticket','Cabin')))],
             function(x) as.factor(x)
      ))
  RMS.TEST$CHILD  <- as.numeric(RMS.TEST$CHILD)
  RMS.TEST$MOTHER <- as.numeric(RMS.TEST$MOTHER) 
  PREDICTION.TEST <- data.frame(Survived = predict(RF_2, RMS.TEST))
  RESULT <- data.frame(PassengerId = RMS.TEST$PassengerId, Survived = PREDICTION.TEST$Survived)
  write.csv(RESULT, "170701_RMS_ITER2.csv", row.names = F)
  #Train accuracy - 93.9%
  #Leaderboard score - 0.78469 ITER 1
  #                  - 0.78947 ITER 2 - Included Mother and Child
  #Rank - 2289
  
  {
    leaf_sizes <- c(5,10,15,20,30,50,100,200)
    for (leaf_size in c(2,3,4,5,6)) {
      RF_2        <- randomForest(Survived ~ Pclass+
                                    Sex+
                                    Age+
                                    SibSp+
                                    Parch+
                                    Fare+
                                    Embarked+
                                    Title+
                                    CHILD+
                                    MOTHER+
                                    F.SIZE,
                                  data = data.frame(RMS.TRAIN),
                                  importance = T,
                                  ntree = 1000,
                                  mtry = as.integer(ncol(RMS.TRAIN)/leaf_size),
                                  nodesize = 15
                                  )
      #print(RF_2$confusion)
      PREDICTION.TEST <- data.frame(Survived = predict(RF_2, RMS.TRAIN))
      table_ = as.matrix(table(PREDICTION.TEST$Survived, RMS.TRAIN$Survived))#93.9%
      print((table_[1,1]+table_[2,2])/(table_[1,1]+table_[2,2]+table_[1,2]+table_[2,1]))
    }
  }#Paramter tuning
  
  #Tune mtry and node size for better results    
  RF_2        <- randomForest(Survived ~ Pclass+
                                Sex+
                                Age+
                                SibSp+
                                Parch+
                                Fare+
                                Embarked+
                                Title+
                                CHILD+
                                MOTHER+
                                F.SIZE,
                              data = data.frame(RMS.TRAIN),
                              importance = T,
                              ntree = 1000,
                              #Tuning
                              #mtry = as.integer(ncol(RMS.TRAIN)/2),
                              nodesize = 15
  )
  PREDICTION.TEST <- data.frame(Survived = predict(RF_2, RMS.TEST))
  RESULT <- data.frame(PassengerId = RMS.TEST$PassengerId, Survived = PREDICTION.TEST$Survived)
  write.csv(RESULT, "170701_RMS_ITER3.csv", row.names = F)
  
  {
    set.seed(009)
    k = 10
    RMS.TRAIN$ID <- sample(1:k, nrow(RMS.TRAIN), replace = T)
    error        <- numeric()
    for (i in 1:k) {
      TRAIN <- RMS.TRAIN[RMS.TRAIN$ID != i,][,!(colnames(RMS.TRAIN) == "ID")]
      TEST  <- RMS.TRAIN[RMS.TRAIN$ID == i,][,!(colnames(RMS.TRAIN) == "ID")]
      RF_2        <- randomForest(Survived ~ Pclass+
                                    Sex+
                                    Age+
                                    SibSp+
                                    Parch+
                                    Fare+
                                    Embarked+
                                    Title+
                                    CHILD+
                                    MOTHER+
                                    F.SIZE,
                                  data = data.frame(TRAIN),
                                  importance = T,
                                  ntree = 1000,
                                  #Tuning
                                  #mtry = as.integer(ncol(TRAIN)/2),
                                  nodesize = 15
      )
      PREDICTION.TEST <- data.frame(Survived = predict(RF_2, TEST))
      table_ = as.matrix(table(PREDICTION.TEST$Survived, TEST$Survived))
      print((table_[1,1]+table_[2,2])/(table_[1,1]+table_[2,2]+table_[1,2]+table_[2,1]))
      error[i] = 1 - (table_[1,1]+table_[2,2])/(table_[1,1]+table_[2,2]+table_[1,2]+table_[2,1])
    }
    bias <- mean(error)
    variance <- sd(error)
    print(paste("Bias = ", bias, " & Variance = ", variance))
    #[1] "Bias =  0.163865568838648  & Variance =  0.0131583755083429"
  }#K fold validation
  
}#11. Modelling technique - Random forest - 2
{
  k = 10
  RMS.TRAIN$ID <- sample(1:k, nrow(RMS.TRAIN), replace = T)
  error        <- numeric()
  for (i in 1:k) {
    TRAIN <- RMS.TRAIN[RMS.TRAIN$ID != i,][,!(colnames(RMS.TRAIN) == "ID")]
    TEST  <- RMS.TRAIN[RMS.TRAIN$ID == i,][,!(colnames(RMS.TRAIN) == "ID")]
    RF_2        <- randomForest(Survived ~ Pclass+
                                  Sex+
                                  Age+
                                  SibSp+
                                  Parch+
                                  Fare+
                                  Embarked+
                                  Title+
                                  CHILD+
                                  MOTHER+
                                  F.SIZE,
                                data = data.frame(TRAIN),
                                importance = T,
                                ntree = 2000,
                                #Tuning
                                #mtry = as.integer(ncol(TRAIN)/2),
                                nodesize = 20
    )
    PREDICTION.TEST <- data.frame(Survived = predict(RF_2, TEST))
    table_ = as.matrix(table(PREDICTION.TEST$Survived, TEST$Survived))#93.9%
    print((table_[1,1]+table_[2,2])/(table_[1,1]+table_[2,2]+table_[1,2]+table_[2,1]))
    error[i] = 1 - (table_[1,1]+table_[2,2])/(table_[1,1]+table_[2,2]+table_[1,2]+table_[2,1])
  }
  bias <- mean(error)
  variance <- sd(error)
  print(paste("Bias = ", bias, " & Variance = ", variance))
}#12. K fold cross validation
{
  #Module 03, 04, 09, 05
  #Filter out unrequired columns - 
  REDUCE_COLS <- c("PassengerId","Name","Ticket","Cabin","F.SIZE.B")
  RMS.TRAIN.7 <- data.table(RMS.TRAIN.7)
  RMS.TRAIN.7 <- RMS.TRAIN.7[, !REDUCE_COLS, with = FALSE] 
  FEATURES_OHE<- c("Pclass",
                   "Sex",
                   "Embarked",
                   "Title",
                   "CHILD",
                   "MOTHER"
  )
  DUMMIES     <- dummyVars(~ Pclass+
                             Sex+
                             Embarked+
                             Title+
                             CHILD+
                             MOTHER,
                           data = RMS.TRAIN.7)
  RMS.TRAIN.7.OHE <- as.data.frame(predict(DUMMIES, newdata = RMS.TRAIN.7))
  RMS.TRAIN.7     <- data.frame(RMS.TRAIN.7)
  RMS.TRAIN.7.ENC <- cbind(RMS.TRAIN.7[,-c(which(colnames(RMS.TRAIN.7) %in% FEATURES_OHE))], RMS.TRAIN.7.OHE)
  RMS.TRAIN.7.ENC[] <- lapply(RMS.TRAIN.7.ENC, as.numeric)
  RMS.TRAIN.7.ENC.F <- RMS.TRAIN.7.ENC[,which(colnames(RMS.TRAIN.7.ENC) != "Survived")] 
  #Run XGB
  set.seed(1007)
  XGB_1 <- xgboost(data = data.matrix(RMS.TRAIN.7.ENC.F),
                   label = RMS.TRAIN.7.ENC$Survived,
                   eta = 0.01,
                   max_depth = 5,
                   nround = 2000,
                   seed = 1007,
                   objective = "binary:logistic",
                   nthread = 2
  )
  #test
  RMS.TRAIN.3 <- data.table(RMS.TRAIN.3)
  RMS.TRAIN.3 <- RMS.TRAIN.3[, !REDUCE_COLS, with = FALSE] 
  FEATURES_OHE<- c("Pclass",
                   "Sex",
                   "Embarked",
                   "Title",
                   "CHILD",
                   "MOTHER"
  )
  DUMMIES     <- dummyVars(~ Pclass+
                             Sex+
                             Embarked+
                             Title+
                             CHILD+
                             MOTHER,
                           data = RMS.TRAIN.3)
  RMS.TRAIN.3.OHE <- as.data.frame(predict(DUMMIES, newdata = RMS.TRAIN.3))
  RMS.TRAIN.3     <- data.frame(RMS.TRAIN.3)
  RMS.TRAIN.3.ENC <- cbind(RMS.TRAIN.3[,-c(which(colnames(RMS.TRAIN.3) %in% FEATURES_OHE))], RMS.TRAIN.3.OHE)
  RMS.TRAIN.3.ENC[] <- lapply(RMS.TRAIN.3.ENC, as.numeric)
  PREDICTION <- predict(XGB_1, data.matrix(RMS.TRAIN.3.ENC))
  PREDICTION <- data.frame(PREDICTION = as.numeric(PREDICTION > 0.5))
  confusionMatrix(PREDICTION$PREDICTION, RMS.TRAIN.3.ENC$Survived)
  
  
  REDUCE_COLS <- c("PassengerId","Name","Ticket","Cabin","F.SIZE.B")
  RMS.TRAIN <- data.table(RMS.TRAIN)
  RMS.TRAIN <- RMS.TRAIN[, !REDUCE_COLS, with = FALSE] 
  FEATURES_OHE<- c("Pclass",
                   "Sex",
                   "Embarked",
                   "Title",
                   "CHILD",
                   "MOTHER"
  )
  DUMMIES     <- dummyVars(~ Pclass+
                             Sex+
                             Embarked+
                             Title+
                             CHILD+
                             MOTHER,
                           data = RMS.TRAIN)
  RMS.TRAIN.OHE <- as.data.frame(predict(DUMMIES, newdata = RMS.TRAIN))
  RMS.TRAIN     <- data.frame(RMS.TRAIN)
  RMS.TRAIN.ENC <- cbind(RMS.TRAIN[,-c(which(colnames(RMS.TRAIN) %in% FEATURES_OHE))], RMS.TRAIN.OHE)
  RMS.TRAIN.ENC[] <- lapply(RMS.TRAIN.ENC, as.numeric)
  RMS.TRAIN.ENC.F <- RMS.TRAIN.ENC[,which(colnames(RMS.TRAIN.ENC) != "Survived")]
  {
    k = 10
    RMS.TRAIN.ENC.F$ID <- sample(1:k, nrow(RMS.TRAIN), replace = T)
    RMS.TRAIN.ENC$ID   <- RMS.TRAIN.ENC.F$ID
    error        <- numeric()
    for (i in 1:k) {
      TRAIN <- RMS.TRAIN.ENC.F[RMS.TRAIN.ENC.F$ID != i,][,!(colnames(RMS.TRAIN.ENC.F) == "ID")]
      TEST  <- RMS.TRAIN.ENC.F[RMS.TRAIN.ENC.F$ID == i,][,!(colnames(RMS.TRAIN.ENC.F) == "ID")]
      TRAIN.DEP <- RMS.TRAIN.ENC[RMS.TRAIN.ENC$ID != i,][,!(colnames(RMS.TRAIN.ENC) == "ID")]
      TEST.DEP  <- RMS.TRAIN.ENC[RMS.TRAIN.ENC$ID == i,][,!(colnames(RMS.TRAIN.ENC) == "ID")]
      set.seed(1007)
      XGB_1 <- xgboost(data = data.matrix(TRAIN),
                       label = TRAIN.DEP$Survived,
                       eta = 0.01,
                       max_depth = 5,
                       nround = 2000,
                       seed = 1007,
                       objective = "binary:logistic",
                       nthread = 2,
                       print_every_n = 2000
      )
      PREDICTION.TEST <- predict(XGB_1, data.matrix(TEST))
      PREDICTION.TEST <- data.frame(Survived = as.numeric(PREDICTION.TEST > 0.5))
      #confusionMatrix(PREDICTION$PREDICTION, TEST.DEP$Survived)
      table_ = as.matrix(table(PREDICTION.TEST$Survived, TEST.DEP$Survived))#93.9%
      print((table_[1,1]+table_[2,2])/(table_[1,1]+table_[2,2]+table_[1,2]+table_[2,1]))
      error[i] = 1 - (table_[1,1]+table_[2,2])/(table_[1,1]+table_[2,2]+table_[1,2]+table_[2,1])
    }
    bias <- mean(error)
    variance <- sd(error)
    print(paste("Bias = ", bias, " & Variance = ", variance))
  }#k fold cross validation
  
}#13. Modelling technique - XGBoost - 3
