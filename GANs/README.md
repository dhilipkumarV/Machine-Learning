# Generative Adversarial Networks

[GAN: A Beginner’s Guide to Generative Adversarial Networks](https://deeplearning4j.org/generative-adversarial-network)

  - Generative adversarial networks (GANs) are deep neural net architectures comprised of two nets, pitting one against the other (thus the “adversarial”)
  - **Generative vs. Discriminative Algorithms** : Discriminative algorithms map features to labels. They are concerned solely with that correlation. Generative algorithms do the opposite, instead of predicting a label given certain features, they attempt to predict features given a certain label
      - Discriminative models learn the boundary between classes
      - Generative models model the distribution of individual classes
  - **How GANs Work** : 
      - One neural network, called the *generator*, generates new data instances, while the other, the *discriminator*, evaluates them for authenticity; i.e. the discriminator decides whether each instance of data it reviews belongs to the actual training dataset or not
      - The discriminator network is a standard convolutional network that can categorize the images fed to it, a binomial classifier labeling images as real or fake. The generator is an inverse convolutional network, in a sense: While a standard convolutional classifier takes an image and downsamples it to produce a probability, the generator takes a vector of random noise and upsamples it to an image. The first throws away data through downsampling techniques like maxpooling, and the second generates new data
        <img src = "https://deeplearning4j.org/img/GANs.png">
  - **GANs, Autoencoders and VAEs** : 
      - Autoencoders encode input data as vectors. They create a hidden, or compressed, representation of the raw data. They are useful in dimensionality reduction; that is, the vector serving as a hidden representation compresses the raw data into a smaller number of salient dimensions. Autoencoders can be paired with a so-called decoder, which allows you to reconstruct input data based on its hidden representation, much as you would with a restricted Boltzmann machine
          <img src = "https://deeplearning4j.org/img/autoencoder_schema.jpg">
      - Variational autoencoders are generative algorithm that add an additional constraint to encoding the input data, namely that the hidden representations are normalized. Variational autoencoders are capable of both compressing data like an autoencoder and synthesizing data like a GAN. However, while GANs generate data in fine, granular detail, images generated by VAEs tend to be more blurred
      - You can bucket generative algorithms into one of three types :
          - Given a label, they predict the associated features (Naive Bayes)
          - Given a hidden representation, they predict the associated features (VAE, GAN)
          - Given some of the features, they predict the rest (inpainting, imputation)
    
## Reference

- [GAN: A Beginner’s Guide to Generative Adversarial Networks](https://deeplearning4j.org/generative-adversarial-network)
- [Connecting Generative Adversarial Networks and Actor-Critic Methods](https://arxiv.org/pdf/1610.01945.pdf)
- [Interpreting images as samples from a probability distribution](https://bamos.github.io/2016/08/09/deep-completion/#step-1-interpreting-images-as-samples-from-a-probability-distribution)
- [deeplearning4j](https://github.com/deeplearning4j)
